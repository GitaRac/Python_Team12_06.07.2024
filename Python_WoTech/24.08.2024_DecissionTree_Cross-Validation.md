# Task1: add DecisionTreeClassifier to titanic data predictions.
```py
# Import DecissionTreeClassifier
from sklearn.tree import DecisionTreeClassifier 

#Initialise the Model
model_tree = DecisionTreeClassifier(random_state=42)

#Train the Model
model_tree.fit(X_train, y_train)

#Make Prediction
y_pred_tree = model_tree.predict(X_test)

# Evaluate Accuraccy
from sklearn.metrics import accuracy_score
accuracy_tree = accuracy_score(y_test, y_pred_tree)
print(f'Decision Tree Classifier Accuracy: {accuracy_tree:.3f}')
```
OUTPUT: Decision Tree Classifier Accuracy: 0.754

# Task2 : Investigate what is cross-validation and implement cross-validation on any classification model you prefer on Titanic data. Explain to each other, what do you see.

## What is Cross-Validation?
**Cross-validation** is a technique used to assess the performance of a machine learning model in a more robust way than a single train/test split. It divides the dataset into multiple parts (folds) and trains the model on some of the folds while testing it on the others. This is done multiple times, and the final performance metric (e.g., accuracy) is the average across all the test sets.

The most common type of cross-validation is **K-Fold Cross-Validation**:

- The data is split into K subsets (or "folds").
- The model is trained on K-1 folds and tested on the remaining fold.
- This process is repeated K times, with each fold used as the test set exactly once.

```py
# Set Up K-Fold Cross - Validation
from sklearn.model_selection import KFold, cross_val_score

# We'll use 10-fold cross-validation, which means the data will be split into 10 parts, and the model will be trained and tested 10 times
k_fold = KFold(n_splits=10, shuffle=True, random_state=42)

# Apply Cross-Validation to the Model DecissionTreeClassifier)
accuracy_scores = cross_val_score(model_tree, X, y, cv=k_fold, scoring='accuracy')

# Print the Results:
print(f'Accuracy scores for each fold: {accuracy_scores}')
print(f'Average accuracy across all folds: {accuracy_scores.mean()}')

```
OUTPUT: Accuracy scores for each fold: [0.81111111 0.80898876 0.7752809  0.71910112 0.82022472 0.7752809
 0.73033708 0.82022472 0.75280899 0.82022472]
 Average accuracy across all folds: 0.783358302122347

### Explanation of Cross-Validation Results:
**accuracy_scores:** This array contains the accuracy scores for each fold in the cross-validation process.
**accuracy_scores.mean():** This provides the average accuracy score across all 10 folds. This is a more reliable estimate of the model's performance than just using a single train/test split.

## What to Observe:
### 1. Variation in Accuracy Across Folds:

- The accuracy scores vary between **71.91%** and **82.02%**.
- **Observation:** The variation isn't too big, but we see some fluctuation. For example, in one fold, the accuracy drops to **71.91%**, while in others, it is as high as **82.02%**.
- **Interpretation:** This variation means the model’s performance changes a bit depending on which subset of the data it's tested on. Some parts of the data are easier or harder for the model to predict, which is normal.
### 2. Average Accuracy:

- The average accuracy across all folds is **78.34%**.
- **Observation:** This means the model is generally predicting correctly **78.34%** of the time. That’s pretty good!
- **Interpretation:** For the Titanic dataset (where we’re predicting who survived), **78.34%** accuracy shows that the model has learned a useful pattern between the input data and the target (survival). But, there’s still some room to improve its performance.

### Summary:
- **Variation:** The model’s accuracy changes a little depending on the data, but it’s within a reasonable range. This tells us the model isn’t overfitting (memorizing the data) or underfitting (failing to learn).

- **Average Accuracy:** With an average accuracy of 78.34%, the model is performing decently well on the Titanic dataset. It's making good predictions most of the time, but you could try improving it by tuinng the model or trying more complex models.
